{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "# For extracting\n",
    "import requests\n",
    "from config_env import api_key, g_key\n",
    "import time\n",
    "# For transformation\n",
    "import json\n",
    "\n",
    "#for scraping Guardian article\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "import os\n",
    "import urllib.parse\n",
    "from config_env import username, password\n",
    "dbstring = \"@ds163764.mlab.com:63764/heroku_pptb0bm8\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "uri = f'mongodb://{username}{password}{dbstring}'\n",
    "client = MongoClient(uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = client.heroku_pptb0bm8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cloud_insert(output_list):\n",
    "    uri = f'mongodb://{username}{password}{dbstring}'\n",
    "    client = MongoClient(uri)\n",
    "    db = client.heroku_pptb0bm8\n",
    "    db.news.insert_many(output_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nyt_api(begin_date, end_date, query, pages):\n",
    "    from config_env import api_key\n",
    "    base_url = \"https://api.nytimes.com/svc/search/v2/articlesearch.json?\"\n",
    "    # Store a search term\n",
    "    param = 'fq=news_desk:(Foreign)&fq=source:(The New York Times)'\n",
    "    # Search for articles published between a begin and end date\n",
    "    page = '&page='\n",
    "    query_url = f\"{base_url}{param}&api-key={api_key}&q={query}&facet_field=snippet&begin_date={begin_date}&end_date={end_date}{page}\"\n",
    "    \n",
    "    output_list = []\n",
    "# Retrieve articles\n",
    "\n",
    "    for n in range(0, pages):\n",
    "        url=query_url + str(n)\n",
    "        try:\n",
    "            articles = requests.get(url).json()\n",
    "            print(f\"Getting data from page {n} from NY Times query...\")\n",
    "            articles_list = [article for article in articles[\"response\"][\"docs\"]]\n",
    "\n",
    "            for item in articles_list:\n",
    "                list_dict = {}\n",
    "                list_dict[\"url\"] = item[\"web_url\"]\n",
    "                list_dict[\"headline\"] = item[\"headline\"][\"main\"]\n",
    "                list_dict[\"pub_date\"] = item[\"pub_date\"]\n",
    "                list_dict[\"source\"] = \"NYT\"\n",
    "                output_list.append(list_dict)\n",
    "\n",
    "            time.sleep(6)\n",
    "        except KeyError:\n",
    "            print(f\"Error with {n}\")\n",
    "            pass\n",
    "    cloud_insert(output_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nyt_api('20190102', '20190222', 'Venezuela', 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def guardian_api(begin_date, query, pages):\n",
    "    # Store a search term\n",
    "\n",
    "    url = f\"https://content.guardianapis.com/search?section=world&page-size=100&from-date={begin_date}&q={query}&api-key={g_key}&page=\"\n",
    "    output_list = []\n",
    "    # For the Guardian API\n",
    "    for i in range(1, pages):\n",
    "        query_url = url + str(i)\n",
    "        print(f'Getting data from page {pages} of Guardian query...')\n",
    "        try:\n",
    "            articles = requests.get(query_url).json()\n",
    "            if articles['response']['status'] == 'ok': \n",
    "                articles_list = [article for article in articles[\"response\"][\"results\"]]\n",
    "                for item in articles_list:\n",
    "                    if item[\"type\"] == 'article':\n",
    "                        list_dict = {}\n",
    "                        list_dict[\"url\"] = item[\"webUrl\"]\n",
    "                        list_dict[\"headline\"] = item[\"webTitle\"]\n",
    "                        list_dict[\"pub_date\"] = item[\"webPublicationDate\"]\n",
    "                        list_dict[\"source\"] = \"Guardian\"\n",
    "                        output_list.append(list_dict)\n",
    "        except KeyError:\n",
    "            pass\n",
    "    cloud_insert(output_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def article_text(collection):\n",
    "    nyt = []\n",
    "    guard = []\n",
    "    for item in collection:\n",
    "        if item['source'] == 'NYT': # If this is from the NY Times do this...\n",
    "            url = item['url']\n",
    "            nyt_dict = {}\n",
    "            print(f\"Getting article text for {item['url']}...\") \n",
    "            # Use Requests to get the HTML, verify False gets around https verification\n",
    "            try:\n",
    "                response = requests.get(url, verify=False)\n",
    "                soup = bs(response.text, 'lxml')\n",
    "                # Class specifies the body of the article\n",
    "                body_copy = soup.find('div', class_='css-53u6y8')\n",
    "            except AttributeError:\n",
    "                pass\n",
    "            nyt_dict['body_copy'] = str(body_copy)\n",
    "            # Insert this back into the database matching the URL\n",
    "            db.news.update_one({'url':url}, {'$set': nyt_dict})\n",
    "            time.sleep(1)\n",
    "        \n",
    "        elif item['source'] == 'Guardian':\n",
    "            g_dict = {}\n",
    "            url = item['url']\n",
    "            print(f\"Getting article text for {item['url']}...\") \n",
    "            try:\n",
    "                response = requests.get(url)\n",
    "                soup = bs(response.text, 'html.parser')\n",
    "                results = soup.find('div', class_=\"content__article-body\")\n",
    "            except AttributeError:\n",
    "                pass\n",
    "            text = results.find_all('p')\n",
    "            g_dict['body_copy'] = str(text)\n",
    "            db.news.update_one({'url':url}, {'$set': g_dict})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# guardian_api('2019-01-02', 'Venezuela',10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go in and fill in any empty body text\n",
    "news = db.news.find({'body_copy': { '$exists': False}})\n",
    "article_text(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
